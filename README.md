## llama-runner
Llama.cpp runner/swapper i proxy emulujƒÖce backend LMStudio / Ollama (dla IntelliJ AI Assistant / GitHub Copilot / VS COde za pomocƒÖ RooCode)

# Instalacja

Kompletny poradnik dla WSL2 w Windows 11 opisujƒÖcy instalacjƒô i konfiguracjƒô lokalnego du≈ºego modleu jƒôzyukowego LLM:

* `llamafile` oraz `llama-runner`,
* z modelem 30B Q4 lub Qwen3 14B 128K,
* dzia≈ÇajƒÖcych przez `llama.cpp` z patchem dla 80K kontekstu,
* z integracjƒÖ do RooCode w VS Code,
* uruchamianych przez WSL2 (i dodatkowo przez CMake na Windows jako bonus).

# ≈örodowisko i przygotowanie sprzƒôtu

Komputer **AtomMan X7 Ti** z 96 GB RAM i kartƒÖ **NVIDIA RTX 3060 (12 GB)** pod≈ÇƒÖczonƒÖ przez eGPU DEG1 (OcuLink) jest podstawƒÖ. Pracujemy pod **Windows 11**, ale ca≈Ça instalacja i uruchamianie LLM odbywa siƒô w **WSL2** (Ubuntu 22.04).

* **Sterowniki GPU:** Na Windows zainstaluj sterownik NVIDIA zgodny z WSL2 i CUDA (najlepiej najnowszy z serii 525+). Upewnij siƒô, ≈ºe Windows wykrywa GPU, a w WSL uzyskuje siƒô dostƒôp do GPU (polecenie `nvidia-smi` w WSL2 powinno pokazaƒá RTX 3060). .
* **Zasilacz i eGPU:** 650 W PSU starczy dla RTX 3060. Brak NVLink oznacza, ≈ºe nie rozdzielamy oblicze≈Ñ miƒôdzy karty - ca≈Ço≈õƒá modelu musi zmie≈õciƒá siƒô w jednej karcie (lub czƒô≈õciowo na RAM). W praktyce **gpu\_layers=99** (ponad 98%) stawia wiƒôkszo≈õƒá sieci na GPU, co wymaga kwantyzacji i zarzƒÖdzania pamiƒôciƒÖ, by nie przekroczyƒá 12‚ÄØGB VRAM.

Zainstaluj Windows Subsystem Linux z poziomu PowerShell

```powershell
dism.exe /online /enable-feature /featurename:Microsoft-Windows-Subsystem-Linux /all /norestart
dism.exe /online /enable-feature /featurename:VirtualMachinePlatform /all /norestart
wsl --set-default-version 2
```
W systemie Windows w≈ÇƒÖcz **funkcjƒô WSL2** (PowerShell: `wsl --install`) i dodaj dystrybucjƒô **Ubuntu 22.04** z Microsoft Store. Tak, w Debian 12 nie dzia≈Ça instalacja NVIDIA CUDA üòÇ Nastƒôpnie w WSL:

```bash
sudo apt update
sudo apt install -y build-essential dkms cmake git python3 python3-pip nvidia-cuda-toolkit nvidia-cuda-dev libcurl4-openssl-dev curl jq unzip zipalign
```

Po restarcie uruchom polecenie `nvidia-smi`; je≈õli widzisz listƒô procesor√≥w, GPU jest gotowe.

## Kompilacja llama.cpp z obs≈ÇugƒÖ CUDA i d≈Çugiego kontekstu

W WSL2 pobierz repozytorium llama.cpp:

```bash
cd ~
git clone https://github.com/ggml-org/llama.cpp
cd llama.cpp
```

*Wa≈ºne:* domy≈õlnie llama.cpp obs≈Çuguje do 8K lub 32K kontekstu zale≈ºnie od modelu. Aby uzyskaƒá *80‚ÄØ000 token√≥w*, u≈ºyj mechanizmu RoPE-skali i YaRN (eyrap) - opisane poni≈ºej. Nie ma oficjalnego automatycznego patcha 80K, ale mo≈ºna zmieniƒá ustawienia RoPE i YaRN (podej≈õcie ‚Äûrope-scaling yarn‚Äù).

Zbuduj bibliotekƒô z flagami CUDA:

```bash
cmake -B build -DGGML_CUDA=ON -DGGML_CUDA_FORCE_CUBLAS=ON -DGGML_CUDA_FA_ALL_QUANTS=ON -DGGML_CUDA_F16=ON -DCMAKE_BUILD_TYPE=Release
cmake --build build --config Release -j$(nproc)
```

* `-DGGML_CUDA=ON` w≈ÇƒÖcza akceleracjƒô GPU (cUBLAS i flash-attn).
* `-DGGML_CUDA_FA_ALL_QUANTS=ON` pozwala na pe≈Çne wsparcie wszystkich kombinacji ilo≈õci bit√≥w w pamiƒôci KV dla flash-attn (potrzebne przy mieszaniu Q4/KV). Kompilacja jest wtedy d≈Çu≈ºsza, ale niezbƒôdna dla naszych ustawie≈Ñ `cache-type-k=f16`, `cache-type-v=q4_0`.
* `-DGGML_CUDA_FORCE_CUBLAS=ON` wymusza u≈ºycie cUBLAS do mno≈ºe≈Ñ macierzowych (mo≈ºe przyspieszyƒá na nowszych GPU kosztem wiƒôkszej pamiƒôci).
* `-DGGML_CUDA_F16=ON` umo≈ºliwia u≈ºycie precyzji FP16 w pewnych operacjach CUDA, co poprawia wydajno≈õƒá na nowszych kartach.

Dodatkowo warto ustawiƒá zmienne ≈õrodowiskowe przy uruchomieniu:

```bash
export GGML_CUDA_ENABLE_UNIFIED_MEMORY=1     # fallback do RAM, gdy VRAM zabraknie:contentReference[oaicite:3]{index=3}  
export GGML_CUDA_FORCE_CUBLAS=1             # (na wszelki wypadek)  
```

`GGML_CUDA_ENABLE_UNIFIED_MEMORY=1` pozwala na u≈ºywanie pamiƒôci systemowej, gdy GPU nie mie≈õci ca≈Çego zestawu KV (zapobiega zrywaniu programu przy 80K kontekstu). Bez tego przy tak du≈ºej pamiƒôci KV mo≈ºemy ≈Çatwo przekroczyƒá 12‚ÄØGB VRAM.

Po kompilacji w katalogu `build/bin` powstanƒÖ pliki binarne, w tym `llama-server` (serwer OpenAI-API) i `llama-cli` (narzƒôdzie CLI). Upewnij siƒô, ≈ºe program poprawnie widzi GPU (`./build/bin/llama-cli --version` powinien pokazaƒá twojƒÖ kartƒô RTX 3060).

## Metoda 1: **Llama-runner (lokalny serwer OpenAI-API z llama.cpp)**

Ta metoda to uruchomienie standardowego serwera z projektu llama.cpp (kompatybilnego z OpenAI). Oferuje pe≈ÇnƒÖ kontrolƒô i konfiguracjƒô GGUF. Postƒôpujemy tak:

1. **Przygotowanie modelu:** Jak wy≈ºej, pobierz wybrany model GGUF (30B Q4 lub Qwen3 14B) i umie≈õƒá np. w `/home/user/models/model.gguf`.

```bash
cd ~
mkdir models && cd models
wget -O Qwen3-14B-128K-IQ4_NL.gguf https://huggingface.co/unsloth/Qwen3-14B-128K-GGUF/resolve/main/Qwen3-14B-128K-IQ4_NL.gguf
```

2. **Uruchomienie serwera:** U≈ºyj skompilowanego pliku `llama-server` z katalogu `build/bin` (powsta≈Çego w kroku budowy). Przyk≈Çad:

**Pamiƒôtaj, aby user zastƒÖpiƒá nazwƒÖ swojego u≈ºytkownika domowego!**

```bash
cd llama.cpp
./build/bin/llama-server \
  -m /home/user/models/Qwen3-14B-128K-IQ4_NL.gguf \
  --ctx-size 80000 \
  --gpu-layers 99 \
  --no-kv-offload \
  --flash-attn \
  --cache-type-k f16 \
  --cache-type-v q4_0 \
  --rope-scaling yarn \
  --rope-scale 4 \
  --jinja  --yarn-orig-ctx 32768 \
  --port 8080
```

   Tutaj:

   * `-m` wskazuje ≈õcie≈ºkƒô do modelu GGUF,
   * reszta flag (`--ctx-size`, `--gpu-layers`, itd.) identycznie jak wy≈ºej,
   * `--port 8080` ustawia nas≈Çuch na porcie 8080 (mo≈ºna zmieniƒá).

   Serwer startuje i czeka na zapytania na `http://localhost:8080/v1/chat/completions` i innych ko≈Ñc√≥wkach OpenAI. Mo≈ºesz go testowaƒá (np. `curl` z JSONem, patrz \[70] lub \[120] dla przyk≈Çadu struktury API).

3. **Test lokalny (opcjonalnie):** Mo≈ºesz sprawdziƒá dzia≈Çanie przez `curl` (opcja `-d` JSON z wiadomo≈õciami jak w OpenAI) lub za pomocƒÖ narzƒôdzi typu Postman. Przyk≈Çadowo:

   ```bash
   curl http://localhost:8080/v1/chat/completions \
     -H "Content-Type: application/json" \
     -d '{
           "model": "any-model",
           "messages": [
             {"role": "system", "content": "Jeste≈õ asystentem kodu."},
             {"role": "user", "content": "Napisz funkcjƒô w Pythonie sortujƒÖcƒÖ listƒô liczb."}
           ]
         }'
   ```

   Model zwr√≥ci JSON z odpowiedziƒÖ pod kluczem `"choices":[{"message":{"content":...}}]`.

> **Praktyczne uwagi:** Z racji braku NVLink i ograniczonej VRAM u≈ºycie `--no-kv-offload` i podanych kwantyzacji (`k=f16`, `v=q4_0`) jest kluczowe, by model ‚Äûzmie≈õci≈Ç siƒô‚Äù w 12 GB. W≈ÇƒÖczenie **FlashAttention** (`--flash-attn`) przyspieszy obliczenia na GPU. Je≈õli zauwa≈ºysz b≈Çƒôdy pamiƒôci, spr√≥buj tak≈ºe zmieniƒá `GGML_CUDA_FORCE_MMQ` na `1` (w ≈õrodowisku), co zmniejszy zu≈ºycie VRAM kosztem nieco wolniejszych oblicze≈Ñ. Upewnij siƒô te≈º, ≈ºe masz w≈ÇƒÖczonƒÖ obs≈Çugƒô *Unified Memory* (`GGML_CUDA_ENABLE_UNIFIED_MEMORY=1`), jak pokazano wy≈ºej.

## Metoda 2: **llama-runner** (samodzielne uruchomienie modelu w spos√≥b bardziej uproszczony)

Ta metoda to uruchomienie runnera z projektu [llama-runner ](https://github.com/sysadmin-info/llama-runner.git). Jest to fork projektu [llama-runner by Piotr Wilkin](https://github.com/pwilkin/llama-runner.git), kt√≥ry zosta≈Ç zmodyfikowany przeze mnie, aby m√≥c uruchamiaƒá go z poziomu WSL2.

Zak≈Çadam, ≈ºe wszystkie kroki przed metodƒÖ pierwszƒÖ zosta≈Çy wykonane.

Sklonuj repozytorium:

```bash
git clone https://github.com/sysadmin-info/llama-runner.git
cd llama-runner
mkdir dev-venv
python3 -m venv dev-venv
source dev-venv/bin/activate
pip install -r requirements.txt
python3 main.py
```


## Integracja z VS Code (RooCode)

Aby korzystaƒá z modelu jako backendu LLM w VS Code (np. z rozszerzeniem **RooCode**, dawniej RooCline):

1. **Zainstaluj RooCode:** Wejd≈∫ w Extensions w VS Code i zainstaluj ‚ÄûRoo Code‚Äù autorstwa RooVeterinaryInc. Po instalacji pojawi siƒô ikona RooCode w panelu bocznym.

2. **Konfiguracja po≈ÇƒÖczenia:** RooCode pozwala pod≈ÇƒÖczyƒá siƒô do dowolnego ‚ÄûOpenAI-compatible‚Äù endpointu. W ustawieniach (lub pliku `settings.json` VS Code) wpisz:

   ```json
   "roocode.openai_api_base": "http://localhost:8080/v1",
   "roocode.openai_model": "qwen3-14b-128k",   // nazwa modelu mo≈ºe byƒá dowolna, serwer ignoruje tƒô warto≈õƒá bo bierze jƒÖ z config.json
   "roocode.openai_api_key": ""
   ```

   * `openai_api_base` wskazuje adres Twojego serwera llama.cpp (z plusem ≈õcie≈ºki `/v1`).
   * `openai_model` w RooCode musi byƒá podane, ale serwer llama.cpp nie u≈ºywa tej warto≈õci (mo≈ºe byƒá `any-model`).
   * `openai_api_key` zostaw puste ‚Äì lokalny serwer nie wymaga autoryzacji.

   W niekt√≥rych wersjach mo≈ºe to byƒá w GUI ustawie≈Ñ: wybierz *AI Provider ‚Üí Add Provider ‚Üí Generic OpenAI*, i tam podaj URL `http://localhost:8080` oraz klucz (pusty).

3. **U≈ºywanie:** Po konfiguracji RooCode bƒôdzie wysy≈Çaƒá zapytania do lokalnego serwera, a model odpowiadaƒá jak zdalny. Nie musisz u≈ºywaƒá `curl` ani osobnego interfejsu ‚Äì wszystkie zapytania generowane sƒÖ wewnƒôtrznie (np. w oknie czatu RooCode).

> **Uwaga:** W praktyce u≈ºytkownicy reportujƒÖ, ≈ºe konfiguracja RooCode z w≈Çasnym serwerem czasem wymaga manualnego wskazania URL w ustawieniach. Wa≈ºne, by serwer `llama-server` (lub `llamafile --server`) dzia≈Ça≈Ç przed uruchomieniem sesji w VS Code.




# Sample config file

```json
{
  "llama-runtimes": {
    "default": {
      "runtime": "llama-server"
    },
    "ik_llama": {
      "runtime": "/home/user/llama.cpp/build/bin/llama-server",
      "supports_tools": false
    }
  },
  "models": {
    "Qwen3 14B 128K": {
      "model_path": "/home/user/models/Qwen3-14B-128K-IQ4_NL.gguf",
      "llama_cpp_runtime": "default",
      "parameters": {
        "ctx_size": 80000,
        "gpu_layers": 99,
        "no_kv_offload": true,
        "cache-type-k": "f16",
        "cache-type-v": "q4_0",
        "flash-attn": true,
        "min_p": 0,
        "top_p": 0.9,
        "top_k": 20,
        "temp": 0.6,
        "rope-scale": 4,
        "yarn-orig-ctx": 32768,
        "jinja": true
      }
    },
    "Qwen3 8B": {
      "model_path": "/mnt/win/k/models/unsloth/Qwen3-8B-GGUF/Qwen3-8B-Q5_K_M.gguf",
      "llama_cpp_runtime": "default",
      "parameters": {
        "ctx_size": 25000,
        "gpu_layers": 99,
        "cache-type-k": "f16",
        "cache-type-v": "q4_0",
        "flash-attn": true,
        "min_p": 0,
        "top_p": 0.9,
        "top_k": 20,
        "temp": 0.6,
        "no_webui": true
      }
    },
    "Qwen3 30B MoE": {
      "model_path": "/mnt/win/k/models/unsloth/Qwen3-30B-A3B-GGUF/Qwen3-30B-A3B-UD-Q3_K_XL.gguf",
      "llama_cpp_runtime": "default",
      "parameters": {
        "override_tensor": "(up_exps|down_exps)=CPU",
        "ctx_size": 22000,
        "gpu_layers": 99,
        "min_p": 0,
        "top_p": 0.9,
        "top_k": 20,
        "temp": 0.6
      }
    },
    "Hermes 3B": {
      "model_path": "/mnt/win/k/models/NousResearch/Hermes-3-Llama-3.2-3B-GGUF/Hermes-3-Llama-3.2-3B.Q5_K_M.gguf",
      "llama_cpp_runtime": "default",
      "parameters": {
        "ctx_size": 100000,
        "gpu_layers": 99,
        "cache-type-k": "q8_0",
        "cache-type-v": "q8_0",
        "flash-attn": true
      }
    }
  }
}
```

# Functionality
* support for different llama.cpp runtimes including ik_llama (for ik_llama, specify "port" in model configuration for runner)
* dynamically loads and unloads runtimes based on model string in request
* dynamically strips tool queries for ik_llama that doesn't support it
* double proxy: emulation for LM Studio-specific backend and OpenAI-compatible backends (running on port 1234) and for Ollama specific backends (running on port 11434)
* tested on GitHub Copilot (for Ollama emulation) and on IntelliJ AI Assistant (for LM Studio emulation)
* tested on Windows & Linux (Ubuntu 24.10)

# Disclaimer

Yes, this is mostly vibe-coded. Pull requests fixing glaring code issues / inefficiencies are welcome. Comments pointing out glaring code issues / inefficiencies are not welcome (unless it's security-critical).
